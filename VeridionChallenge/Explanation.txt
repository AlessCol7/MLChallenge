After analyzing the data and the documents provided, I began thinking about similar challenges Iâ€™ve worked on before. For natural language processing, I initially used GloVe and later enhanced it with an LSTM. However, this approach didn't seem suitable for this task, mainly because the data wasn't labeled.
I started by downloading the necessary libraries and preprocessing the data. Then, I applied GloVe, which is designed to capture the semantic relationships between words to provide more accurate classifications. Upon testing, I noticed that out of the 220 available labels, the model only selected three labels, with a majority focused on Oil Manufacturing.
Next, I explored different combinations of dataset columns to see which might lead to a more accurate classification. Unfortunately, these adjustments did not yield better results, as presented in the notebook. I also tested a Random Forest Classifier and a Neural Network on top of GloVe, but the classification results worsened, with the model outputting binary classifications despite using a softmax activation function.
When I tried applying an LSTM, I switched to using TF-IDF instead of GloVe. TF-IDF is particularly effective in capturing word frequency and calculating the inverse document frequency, which seemed better suited for this dataset, especially considering the importance of business tags. After running the model and analyzing the results, I found that TF-IDF provided more diverse and accurate label assignments. When 
I applied the LSTM on top of the TF-IDF it gave unsatisfying result, foccusing on generalising the data and giving only a couple of insurance labels that I tested in the Trials.ipynb.
In conclusion, the results presented in the file VeridionChallenge/company_insurance_classification.csv represent the most accurate classification I was able to achieve.

