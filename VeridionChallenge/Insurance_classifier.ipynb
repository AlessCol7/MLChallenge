{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alessiacolumban/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alessiacolumban/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alessiacolumban/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/alessiacolumban/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/alessiacolumban/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append(\"/usr/local/share/nltk_data\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')  # In case it's needed\n",
    "nltk.download('wordnet')  # If lemmatization is used\n",
    "nltk.download('omw-1.4')  # Sometimes needed for WordNet\n",
    "nltk.download('averaged_perceptron_tagger')  # If POS tagging is used\n",
    "\n",
    "# Load company dataset\n",
    "company_df = pd.read_csv(\"ml_insurance_challenge.csv\")  # Adjust file path as needed\n",
    "\n",
    "# Load insurance taxonomy labels\n",
    "insurance_labels_df = pd.read_excel(\"insurance_taxonomy.xlsx\", header=None)\n",
    "insurance_labels = insurance_labels_df[0].tolist()  # Extract labels as a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings: 400000it [00:20, 19220.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded. Example vector for 'insurance': [-5.6131e-02  1.3407e-01 -2.0990e-01  6.6576e-01  3.9511e-01  1.1705e-01\n",
      "  5.9482e-01 -4.4346e-01 -6.3459e-01 -1.4874e+00  1.4473e-01 -4.0983e-02\n",
      "  2.9000e-01 -6.2229e-01  7.6048e-02  2.9911e-02  4.4002e-01 -1.4550e-01\n",
      "  1.4908e-01 -2.0335e-01  6.1897e-01  4.3210e-02 -6.5998e-02  4.8003e-01\n",
      "  1.2430e-01  3.2287e-01 -2.0818e-01 -3.4093e-02 -4.3413e-01 -1.1550e-01\n",
      " -3.7614e-01  4.4211e-01  2.9823e-01 -4.9800e-01 -1.0069e+00 -1.5966e-01\n",
      "  3.6354e-01 -3.1454e-01  6.5463e-01 -3.5102e-02 -1.1068e-01 -5.7771e-01\n",
      "  5.0527e-01  4.2670e-01  1.2369e-01 -6.6690e-01 -4.9843e-01  4.4435e-01\n",
      " -2.0306e-01 -1.2199e-01 -1.9348e-02  4.6638e-01 -1.4160e-01  4.3297e-01\n",
      "  7.8291e-02 -1.7717e-01  5.8133e-02  4.6367e-01  1.3563e-01 -4.2853e-01\n",
      "  5.4427e-01 -7.3813e-02 -1.5729e-01 -3.8912e-02  7.5924e-01  3.2579e-01\n",
      "  2.0219e-01 -4.3475e-01 -1.8684e-01 -5.8086e-01 -2.8047e-01 -2.3834e-02\n",
      "  6.5123e-01  3.4918e-01  2.6611e-01  3.0088e-01 -9.2837e-01 -2.8944e-01\n",
      " -5.2305e-01 -1.9033e-01  1.1912e-01  5.5250e-01 -9.3993e-02  6.2905e-01\n",
      "  2.3182e-01 -1.4085e-01 -1.4841e-01  2.1217e-01 -5.6745e-01 -1.3239e-01\n",
      " -1.2779e-01  7.9958e-01 -5.8311e-01  1.7230e-02  2.8081e-02  1.4773e-01\n",
      " -4.2979e-01  1.5937e-01  9.1126e-02 -3.4731e-01  1.4675e-01  3.2896e-01\n",
      "  1.5260e-01 -1.8550e-01  3.4871e-01 -2.8213e-01 -2.5802e-01 -4.4421e-01\n",
      "  5.9726e-02  2.4667e-02 -2.4464e-01 -8.9841e-01 -3.0802e-02 -7.1634e-02\n",
      "  4.1824e-01  3.2514e-01  3.7467e-01  2.2555e-01  4.3441e-01  4.5318e-01\n",
      "  7.8711e-01  2.8080e-01  9.6169e-02  3.7721e-01  4.9297e-01 -4.3041e-01\n",
      "  4.4029e-01 -2.0392e-01 -3.8303e-01 -2.3311e-01  5.2368e-01  2.9996e-01\n",
      "  2.0303e-01  4.5774e-01  3.4941e-01 -2.6147e-01 -3.0362e-01 -2.0609e-01\n",
      "  3.3186e-01  1.8439e-01  2.9352e-01  1.8460e-01  4.2063e-01  4.8777e-01\n",
      " -4.3974e-01  4.2021e-01  5.2347e-03  3.8494e-01  7.8046e-01  2.4012e-01\n",
      "  4.2015e-01 -9.0768e-01  5.5279e-01 -7.1599e-01 -4.6079e-01  1.3625e-01\n",
      " -2.7743e-01 -2.6749e-01  1.3930e-01 -4.6467e-01  2.4620e-01 -6.7208e-02\n",
      "  4.6075e-04  2.6852e-01 -1.1009e-01 -1.6545e-02  4.5359e-01 -7.4760e-01\n",
      "  2.9962e-01  3.0970e-01 -1.4849e-01 -9.2995e-02 -5.5722e-01  5.7258e-01\n",
      " -7.5445e-02 -6.7570e-01  2.1701e-01 -4.3490e-01 -1.0343e-01  8.3014e-01\n",
      "  8.3852e-01 -3.1214e-01 -3.1359e-01  1.0758e-01 -2.2635e-01 -2.3625e-01\n",
      "  1.0275e-01 -5.0090e-01  1.1104e-01 -2.3164e-01  6.1200e-01  5.4262e-01\n",
      " -4.0562e-01 -5.1643e-01  1.9149e-01 -4.7914e-01  5.4764e-01  1.7076e-01\n",
      "  5.6355e-02  1.8694e-01  1.8133e-01  4.5820e-01 -1.9356e-01 -5.2107e-02\n",
      "  7.9545e-02  5.8253e-01 -2.0543e-01 -9.2511e-02  2.1268e-01 -1.4173e-01\n",
      " -2.9757e-01 -6.0069e-02 -2.1671e-01 -2.4961e-01  2.0536e-01  5.5778e-02\n",
      " -7.6412e-02  6.4084e-01  5.7684e-01 -1.3864e-01 -1.0605e-01  8.0988e-02\n",
      "  2.8666e-01 -4.2582e-01 -9.9842e-02  2.3758e-01 -4.1853e-01 -4.9910e-02\n",
      "  7.0078e-01  3.2011e-01 -2.3277e-01  3.0834e-01  2.4825e-01  9.5610e-01\n",
      " -4.5104e-02 -1.8387e-01 -2.7050e-01  1.4010e-01 -1.3574e-01  5.6217e-01\n",
      " -3.5286e-02 -2.4343e-01  1.0404e+00  1.4938e-02 -9.1472e-01  4.9385e-02\n",
      "  1.9934e-01 -8.7780e-01  2.7904e-01  2.6482e-01  4.7092e-01  7.8275e-01\n",
      "  2.4844e-01 -8.7446e-02 -1.2466e-01 -5.1414e-01 -1.3059e+00  3.7517e-01\n",
      "  4.1271e-01  6.6480e-02 -2.8102e-01 -5.0816e-01  3.0137e-01  2.6257e-01\n",
      "  1.2368e-01  2.4699e-01 -2.5677e-01 -2.8162e-01 -1.5856e-01  2.4273e-01\n",
      "  1.1474e-01  3.9613e-01 -5.8759e-02  2.0711e-01  3.2070e-01  1.3823e-01\n",
      " -1.5913e+00  1.0364e-01  3.4745e-01  1.9686e-01 -2.4740e-01 -9.8592e-01\n",
      "  2.6252e-01 -9.3233e-02 -5.7724e-02  6.3336e-01  5.2767e-01  4.0392e-01\n",
      "  5.4725e-01 -1.3157e-01  8.9405e-01  1.6488e-01 -2.3070e-02 -7.6472e-01\n",
      "  6.6950e-02  4.2048e-01  3.2916e-01 -1.5570e-01  2.1483e-01  5.0997e-02]\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    glove_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector\n",
    "    return glove_dict\n",
    "\n",
    "# Load GloVe embeddings (change the path if needed)\n",
    "glove_path = \"glove.6B.300d.txt\"  # Ensure this file exists\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "print(\"GloVe embeddings loaded. Example vector for 'insurance':\", glove_embeddings.get(\"insurance\", \"Not found\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_glove_vector(text, glove_embeddings):\n",
    "    words = text.split()  # Simple tokenization\n",
    "    vectors = [glove_embeddings.get(word, np.zeros(300)) for word in words]  # Default to zero vector if word is unknown\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/alessiacolumban/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/share/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokens\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Test preprocessing\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(preprocess_text(\u001b[39m\"\u001b[39;49m\u001b[39mThis is a test sentence for insurance classification!\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32m/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-z0-9\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, text)  \u001b[39m# Remove special characters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokens \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alessiacolumban/Desktop/VeridionChallenge/Insurance_classifier.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lemmatizer \u001b[39m=\u001b[39m WordNetLemmatizer()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[39m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_punkt_tokenizer\u001b[39m(language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_lang(lang)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_lang\u001b[39m(\u001b[39mself\u001b[39m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[39m=\u001b[39m find(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt_tab/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1750\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lang \u001b[39m=\u001b[39m lang\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/alessiacolumban/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/share/nltk_data'\n    - '/Users/alessiacolumban/opt/anaconda3/envs/Yolo_and_tensorflowmetal/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# # Step 3: Preprocess Text Data\n",
    "# def preprocess_text(text):\n",
    "#     if not isinstance(text, str) or pd.isna(text):  # Ensure text is valid\n",
    "#         return []\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# # Test preprocessing\n",
    "# print(preprocess_text(\"This is a test sentence for insurance classification!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove special characters\n",
    "    tokens = text.split()  # Simple alternative to word_tokenize()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_glove_vector(text, embeddings, dim=300):  # Ensure 300D for GloVe 6B.300d\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return np.zeros(dim)  # Return a zero vector if text is missing\n",
    "    \n",
    "    tokens = text.split()  # Using simple tokenization\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)  # Return a zero vector if no words are found\n",
    "    \n",
    "    return np.mean(vectors, axis=0)  # Take the average of word vectors\n",
    "\n",
    "# Convert company descriptions to vectors\n",
    "company_df['glove_vector'] = company_df['description'].apply(lambda x: text_to_glove_vector(x, glove_embeddings))\n",
    "\n",
    "# Convert insurance taxonomy labels to vectors\n",
    "insurance_label_vectors = np.vstack([text_to_glove_vector(label, glove_embeddings) for label in insurance_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrix Shape: (9494, 221)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert list of vectors into a NumPy array\n",
    "company_vectors = np.vstack(company_df['glove_vector'].values)\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(company_vectors, insurance_label_vectors)\n",
    "\n",
    "# Print similarity shape (should be companies × labels)\n",
    "print(f\"Similarity Matrix Shape: {similarity_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Welchcivils is a civil engineering and constru...   \n",
      "1  Kyoto Vegetable Specialists Uekamo, also known...   \n",
      "2  Loidholdhof Integrative Hofgemeinschaft is a c...   \n",
      "3  PATAGONIA Chapa Y Pintura is an auto body shop...   \n",
      "4  Stanica WODNA PTTK Swornegacie is a cultural e...   \n",
      "\n",
      "                                     insurance_label  \n",
      "0  Fishing and Hunting Services, Windows and Door...  \n",
      "1  Fishing and Hunting Services, Windows and Door...  \n",
      "2  Fishing and Hunting Services, Windows and Door...  \n",
      "3  Fishing and Hunting Services, Windows and Door...  \n",
      "4  Fishing and Hunting Services, Windows and Door...  \n"
     ]
    }
   ],
   "source": [
    "def assign_labels(similarity_matrix, labels, top_n=3):\n",
    "    \"\"\"\n",
    "    Assigns the top N most similar labels to each company based on cosine similarity.\n",
    "    \"\"\"\n",
    "    assigned_labels = []\n",
    "    \n",
    "    for row in similarity_matrix:\n",
    "        top_indices = row.argsort()[-top_n:][::-1]  # Get indices of top N labels\n",
    "        best_labels = [labels[i] for i in top_indices]\n",
    "        assigned_labels.append(\", \".join(best_labels))  # Join multiple labels into a string\n",
    "    \n",
    "    return assigned_labels\n",
    "\n",
    "# Assign labels to companies\n",
    "company_df['insurance_label'] = assign_labels(similarity_matrix, insurance_labels, top_n=3)\n",
    "\n",
    "# Display the first few results\n",
    "print(company_df[['description', 'insurance_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results saved to 'classified_companies.csv'\n"
     ]
    }
   ],
   "source": [
    "company_df.to_csv(\"classified_companies.csv\", index=False)\n",
    "print(\"Classification results saved to 'classified_companies.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolo_and_tensorflowmetal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
